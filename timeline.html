<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>Timeline</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="css/bootstrap.css" rel="stylesheet">
    <link href="css/bootstrap-responsive.css" rel="stylesheet">
    <link href="css/docs.css" rel="stylesheet">
    <link href="css/prettify.css" rel="stylesheet">
</head>

<body data-spy="scroll" data-target=".subnav" data-offset="50">

    <!-- Navbar
    ================================================== -->
    <div class="navbar navbar-fixed-top">
        <div class="navbar-inner">
            <div class="container">
                <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </a>
                <a class="brand" href="./index.html">YouMote</a>
                <div class="nav-collapse">
                    <ul class="nav">
                        <li class="">
                            <a href="./index.html">Overview</a>
                        </li>
                        <li class="active">
                            <a href="./timeline.html">Timeline</a>
                        </li>
                        <li class="">
                            <a href="./team.html">Team</a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </div>

    <div class="container">

        <!-- Masthead
        ================================================== -->
        <header class="jumbotron subhead" id="overview">
            <h1>Timeline</h1>
            <p class="lead">Follow the progress of the project on a weekly basis.</p>
            <div class="subnav">
                <ul class="nav nav-pills">
                    <li><a href="#week4">Week 4</a></li>
                    <li><a href="#week5">Week 5</a></li>
                    <li><a href="#week6">Week 6</a></li>
                    <li><a href="#week7">Week 7</a></li>
                    <li><a href="#week8">Week 8</a></li>
                    <li><a href="#week9">Week 9</a></li>
                    <li><a href="#week10">Week 10</a></li>
                </ul>
            </div>
        </header>

        <!-- Week 4
        ================================================== -->
        <section id="week4">
            <div class="page-header">
                <h1>Week 4 <small>Milestone 1: Team Formation and Application Area</small></h1>
            </div>

            <h3>Brainstorming - Take 1</h3>
            <div class="row">
                <div class="span8 columns">
                    <p>We had an intense brainstorming session in class where we covered a wide range of topics. The main fields that our brainstorming evolved at were: cooking, medical, communication, education, law enforcement, art, sports, fun and house.</p>
                    <h4>Problem Area and User Needs - 3D Model Manipulation</h4>
                    <p>The application area we chose to tackle is 3D model navigation and manipulation for designers and architects. The current solution for viewing models on a screen involves rotating, panning and zooming using the mouse and the keyboard. However the controls for manipulating the different axes using the current hardware is challenging and counter-intuitive. Even though with enough time experts in the field get used to the idiosyncrasies of the controller mapping, the state-of-the-art system offers a discouraging learning curve for novices, or the clients the experts would have to interact with.</p>
                    <p>We think that exploring the design space for more natural methods using the Kinect will open up possibilities for navigating and manipulating virtual models. We think that using the depth information available with the Kinect sensor, we can produce methods with much better “walk-up” usability.</p>
                </div>
                <div class="span4">
                    <ul class="thumbnails">
                        <li class="span2">
                            <a href="img/original images/brainstorm.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/brainstorm.jpg"></img>
                            </a>
                        </li>
                    </ul>
                </div>
            </div>

            <h3>Brainstorming - Take 2</h3>
            <div class="row">
                <div class="span8">
                    <p>We decided to get together again to brainstorm ideas on the project scope because our original idea seemed to be more solution-oriented rather than problem-oriented. Furthermore, we weren't able to clearly identify a compelling reason for our target group to pick our gesture-oriented technology over existing alternatives. </p>
                    <p>Our second brainstormed resulted in two main application areas: gesture-oriented TV control for future generation TVs such as Apple TV and Google TV, and sports and fitness applications.</p>
                    <h4>Problem Area and User Needs - Gesture-oriented Interface for TV</h4>
                    <p>Ever since Apple TV and Google TV were announced, the TV industry has shifted its products to a more interactive experience for the users rather than maintaining its traditional role as passively providing information. We are looking at various areas where the user can interface the TV without using any device. For example, one area that is interesting is text-input. Current solutions involve using a keyboard, which is very inconvenient.</p>
                </div>
                <div class="span4">
                    <ul class="thumbnails">
                        <li class="span2">
                            <a href="img/original images/brainstorm2.jpg" class="thumbnail" target='_blank'>			
                                <img src="img/thumbnail images/brainstorm2.jpg"></img>
                            </a>
                        </li>
                        <li class="span2">
                            <a href="img/original images/brainstorm2-questions.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/brainstorm2-questions.jpg"></img>
                            </a>
                        </li>
                    </ul>
                </div>
            </div>

            <h3>Brainstorming - Take 3</h3>
            <div class="row">
                <div class="span8">
                    <p>We got together again to narrow down the application area within the TV space. We also planned out the questions we plan on asking Mr. Jeff Tang from Sony Electronics. The attached photos shows the questions we planned on asking. We then listed out potential application areas within the TV space, and narrowed down to 3 possible areas: TV-personalization, web-browsing, interactive commercials, and social TV.</p>

                    <h4>Problem Area and User Needs - Gesture-oriented Interface for TV</h4>
                    <p>After our skype call with Mr. Tang and further discussion amongst ourselves, we decided to prototype on personalized TVs and text-entry for future TVs.</p>
                </div>
                <div class="span4">
                    <ul class="thumbnails">
                        <li class="span2">
                            <a href="img/original images/questions.jpg" class="thumbnail" target='_blank'>			
                                <img src="img/thumbnail images/questions.jpg"></img>
                            </a>
                        </li>
                        <li class="span2">
                            <a href="img/original images/tv-app-area.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/tv-app-area.jpg"></img>
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
        </section>



        <!-- Week 5
        ================================================== -->
        <section id="week5">
            <div class="page-header">
                <h1>Week 5 <small>Milestone 2: Initial Prototyping</small></h1>
            </div>

            <h3>Meeting with Sony</h3>
            <div class="row">
                <div class="span8">
                    <p>
                        <a href="https://docs.google.com/document/d/19BJjeMf4FPmeZhdMZmNxfirM-_TX2BWUcpmVyoxcs2Y/edit">Meeting notes on Google Docs</a>
                    </p>
                </div>
                <div class="span4">
                    <ul class="thumbnails">
                        <li class="span2">
                            <a href="img/original images/jeff1.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/jeff1.jpg"></img>
                            </a>
                        </li>
                        <li class="span2">
                            <a href="img/original images/jeff2.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/jeff2.jpg"></img>
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
            <h3>Prototype Areas</h3>
            <div class="row">
                <div class="span8">
                    <p>We brainstormed in class on the application areas we wanted to prototype, and ended up with 3 areas: interactive TV, navigation, and personalization.</p>
                </div>
                <div class="span2">
                </div>
                <div class="span2">
                    <ul class="thumbnails">
                        <li class="span2">
                            <a href="img/original images/brainstormforprototype.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/brainstormforprototype.jpg"></img>
                            </a>
                        </li>
                    </ul>
                </div>
            </div>

            <h3>Personalized TV - Storyboard 1</h3>
            <div class="row">
                <div class="span8 columns">
                    <h4>Use Case</h4>
                    <p>This first storyboard explores one possible use case for a TV that is aware of its user and his/her taste profile. Users record signature gestures and can then use these gestures to sign-in to the TV.</p>
                    <p>Once the user is signed in, the TV will switch to the content that best fits the current user's taste profile. The user can then indicate feedback by giving a thumbs up or thumbs down, both of which are saved to enhance future recommendations. The user can get a new show that they like and is on at that time with the thumbs down gesture. Also, if more than one user is present, the TV will find a show that best fits both profiles.</p>
                    <h4>Lessons Learned</h4>
                    <p>Some of the learnings we got from this prototype include limiting of interactivity. With the TV deciding on the list of shows to display, the user doesn't have the capability to navigate to a new, different show that he or she might have just found out about. However this can be overcome by giving an alternative input method.</p>

                </div>
                <div class="span2">
                    <ul class="thumbnails">
                        <li class="span2">
                            <a href="img/original images/personalized_arda.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/personalized_arda.jpg"></img>
                            </a>
                        </li>
                    </ul>
                </div>
            </div>

            <h3>Personalized TV - Storyboard 2</h3>
            <div class="row">
                <div class="span8 columns">
                    <h4>Use Case</h4>
                    <p>
                        This first storyboard explores one possible use case for a TV that is aware of the user's presence and user's  taste profile. In this instance however, the Kinect-TV is passively detecting and recognizing subjects infront of the TV and learning what shows the user likes to watch and suggesting appropriate shows for them 
                    </p>

                    <ol style="list-style-type: upper-latin;">
                        <li>TV is off.</li>
                        <li>Mom walks in front of screen, Kinect detects her and TV turns on.</li>
                        <li>Kinect does facial and body recognition. It recognizes Mom and welcomes her.</li>
                        <li>Knowing what shows Mom likes, the Kinect-TV suggests shows that are currently playing that mom likes. She has the option of Martha Stewart or CNN News. She can point left for Martha Stewart and right for CNN News.</li>
                        <li>Mom Points right and the Kinect changes the channel to CNN.</li>
                        <li>Mom watches the show for 30 minutes.</li>
                        <li>Mom's show is done and the Kinect-TV suggests next shows that are own that she might like.</li>
                        <li>As this happens her son comes into the room.</li>
                        <li>The Kinect-TV detects the son and mom are watching tv.</li>
                        <li>The Kinect-TV runs body and facial recognition on the subjects.</li>
                        <li>The TV welcomes both mom and son and gives suggestions for current shows that they both like to wath together.</li>
                        <li>Mom and son are happy.</li>
                    </ol>
                    <h4>Lessons Learned</h4>
                    <p>Privacy:  In looking at the passive detection of people in front of the screen a lot of users might find this a bit intrusive, like they are being watched by the tv.  Many people might not like that through this passive mode, data is being collected about them all the time.</p>
                    <p>Detection: With passive detection, this can lead to situations when multiple people walk in front of the screen but only 1 person is actually interested in watching tv.</p>
                    <p>Multiple Persons:  With multiple people, tv watchers might not want to watch their group shows and instead want to watch an individual's show but still have two people watching tv.  This kinect-TV mode should be something activated by the tv watchers and not forced on them to watch shows that are only suggested to them.</p>

                </div>
                <div class="span4">
                    <ul class="thumbnails">
                        <li class="span4">
                            <a href="img/original images/personalized_blake1.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/personalized_blake1.jpg"></img>
                            </a>
                        </li>
                        <li class="span4">
                            <a href="img/original images/personalized_blake2.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/personalized_blake2.jpg"></img>
                            </a>
                        </li>
                    </ul>
                </div>
            </div>

            <h3>Personalized TV - Storyboard 3</h3>
            <div class="row">
                <div class="span8 columns">
                    <h4>Use case</h4>
                    <p>This third storyboard explores the possibility of the user using gesture to perform text-input. The user is watching LOST but wants to change a show. By setting his TV into text-input-mode, hec an start to move his hand to perform the text-input. When the user moves his right hand into an upside-down "V"-shape, the TV automatically does fuzzy matches to match it with "A" and "N", amongother letters (which he can scroll down to from the right). He then moves his hand in a circle, and the TV matches his hand to a "D" and an "O", among others. Lastly, he moves his hand into an "8" shape, and the TV matches it with "8" and "B". He scrolls through to find his favorite show, "No. 8", and enjoys the show.</p>
                    <h4> Lessons Learned</h4>
                    <p>Clutch: In order to avoid false positives, the user will need to use some sort of clutch to letthe TV know when the "text-input-mode" is on. The clutch could either be done with a remote or with the left hand.</p>
                    <p>It may be a good idea to explore the possibility of only using gesture-controlled features for text-input and leaving everything else to the remote. It seems like almost every other function in the TV is a lot easier with a remote or a smartphone rather than gestures. However, with text-input, using a remote to select letters is extremely painful, so as gesture-based input makes sense.</p>
                </div>
                <div class="span4">
                    <ul class="thumbnails">
                        <li class="span2">
                            <a href="img/original images/text_input1.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/text_input1.jpg"></img>
                            </a>
                        </li>
                        <li class="span2">
                            <a href="img/original images/text_input2.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/text_input2.jpg"></img>
                            </a>
                        </li>

                    </ul>
                </div>
            </div>

            <h3>Watching TV - Personal Experience</h3>
            <div class="row">
                <div class="span8 columns">

                    <h4>Rationale</h4>
                    <p>In order to experience what TV watchers are doing and feeling, we did the arduous task of of watching TV for 30 minutes.  We learned the following in relaion to Storyboard 2.</p>
                    <h4> Lessons Learned </h4>
                    <p>TV Watching Positions : People watch TV in a variety of positions: reclining, sitting up, standing, in front of laptops, lying down completely.</p>
                    <p>TV Movement: People get up and down frequently while watching TV.  Leave during commercials for food or to go to the bathroom.  Need to be able to deal with people still watching tv even if they aren't in the kinects view.</p>
                    <p> Even though we watched TV alone, we would still laugh, yell, shout, and emote to the screen, as if someone were actually there.</p>
                    <p> Content: When watching Cable TV, the choices are too many. When watching TV from your computer, you usually go directly toward the show you want. What's the middle ground? Is there a way to make show-selection easier for those who have the 'fear of missing out' on the best show?</p>
                </div>
                <div class="span4">
                    <ul class="thumbnails">
                        <li class="span4">
                            <a href="img/original images/tv_watching_blake.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/tv_watching_blake.jpg"></img>
                            </a>
                        </li>
                    </ul>
                </div>
            </div>

            <h3>Interactive TV--Custom Input and Feedback</h3>			
            <div class="row">
                <div class="span8 columns">
                    <h4>Rationale</h4>
                    <p> One idea for the future of TV was to interact with the content itself. When you're watching a show alone, laughs are accompanied by the sitcom-pre-recorded laughs. What if you could see what other people thought was funny? When if you could tell when other people were yelling at the refs in a sports match? </p>

                    <p> The question that this looks-like prototype addressed is exactly what makes sense for a system like this to look like. Given that TV shows have sound, there would need to be some kind of unobstructive visual feedback. </p>

                    <ul>
                        <li>Voice recognition: Could TVs just recognize your laugh and use that?</li>
                        <li>In-Show comments: At a turning point in the TV show, what if you could easily provide feedback about whether you thought something was awkward, cute, or upsetting?</li>
                        <li>Post-show discussion: During the credits for the show, could you see what others thought of it as well?</li>
                    </ul>

                    <h4>Lessons Learned</h4>

                    <ul>
                        <li>
                            There is no one-interface fits all; different shows use different parts of the screen, so having a single selection interface during TV in general would inhibit the experience for some shows. Programming multiple targets for the show seems tough; implicit gesture for voice may be better than target or text selection.
                        </li>
                        <li>
                            Laughing out loud alone was awkward when no one else was in the room and when no laughter was present in the show.
                        </li>
                    </ul>

                </div>
                <div class="span4">
                    <ul class="thumbnails">
                        <li class="span2">
                            <a href="img/original images/sports_targets2.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/sports_targets2.jpg"></img>
                            </a>
                        </li>
                        <li class="span2">
                            <a href="img/original images/voice_recognition.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/voice_recognition.jpg"></img>
                            </a>
                        </li>
                        <li class="span2">
                            <a href="img/original images/inshow_comments.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/inshow_comments.jpg"></img>
                            </a>
                        </li>
                        <li class="span2">
                            <a href="img/original images/postshow_discussion.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/postshow_discussion.jpg"></img>
                            </a>
                        </li>
                    </ul> 
                </div>
            </div>

            <div class="row">
                <div class="span6">
                    <h3>Sitting Gesture Feasibility Testing</h3>
                    <ul class="thumbnails">
                        <li class="span6">
                            <a href="img/original images/SittingGesture.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/SittingGesture.jpg"></img>
                            </a>
                        </li>
                    </ul>
                </div>
                <div class="span6">
                    <h3>Reclined Gesture Feasibility Testing</h3>
                    <ul class="thumbnails">
                        <li class="span6">
                            <a href="img/original images/ReclinedGesture.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/ReclinedGesture.jpg"></img>
                            </a>
                        </li>
                    </ul>
                </div>
            </div>

            <h4>Lessons Learned</h4>
            <p>We wanted to be able to see how well the Kinect could recognize reclining and relaxed individuals. We realize that the TV is usually a place where people come together and hang out--mostly sitting down. A lot of the current Kinect games ask you to stand up, so we wanted to see what happens if someone gets too close to the camera</p>


            <h3>Interactive TV--Custom Camera Views</h3>

            <p>An idea for the future of TV was the following: let's say there is a live concert on TV. Several cameras are filming it: one for the guitarist, one for the drummer, one for the audience, one for the stage, etc. What if it was possible to choose the camera we want to watch in real time?</p>
            <p>The first question we could ask is: when would that possibility be appropriate? We could think of different application areas: concerts, sports, documentaries, TV reality shows.</p>
            <p>The second is even more interesting here since it was actually possible to make a tangible prototype to find the answer: What is a good way to select camera views?</p>
            <p>More generally, this question has a much larger scope. It is about the selection of almost any object on a screen. We could group those gestures in three categories:</p>
            <ul>
                <li>
                    Pointer selection: the user has to point at the object he wants to select on the screen.
                </li>
                <li>
                    Radial selection: the user makes a gesture towards the zone of the screen where the object he wants to select is.
                </li>
                <li>
                    Drawing selection: if the objects have an index (a number or a letter for example), the user "draws" with his hand this number or letter.
                </li>
            </ul>
            <p>The following videos show the three types of interaction (we recommend you to watch them in full-screen mode).</p>
            <div class="row">
                <div class="span4">
                    <iframe width="368" height="238" src="http://www.youtube.com/embed/64LdW3uf_WQ" frameborder="0" allowfullscreen></iframe>
                </div>
                <div class="span4">
                    <iframe width="368" height="238" src="http://www.youtube.com/embed/W_YRnXwqOcQ" frameborder="0" allowfullscreen></iframe>
                </div>
                <div class="span4">
                    <iframe width="368" height="238" src="http://www.youtube.com/embed/JqHTqntNob0" frameborder="0" allowfullscreen></iframe>
                </div>
            </div>
            <h4>Lessons Learned</h4>
            <ul>
                <li>
                    Pointer selection: this selection may be the more direct and natural, but there is (at least) one big flaw. The first one is that this pointer needs a kind of calibration. When the user points at something on the screen, there is little chance that the pointer actually falls exactly where he expected to. He may have to correct the position of his arm. This is particularly true if the items to select are in a tight vertical list.
                </li>
                <li>
                    Radial selection: an obvious flaw that we could spot as soon as we tested this prototype is that you have to be on a decent sitting position to be able to select a zone of the screen. If you are lying on a side (let's say the right side), it is very difficult to direct your arm to the right. On the contrary, this selection method has a big advantage compared to the previous one: if the expanse of the gesture can be small, it would not only solve the problem of the posture but it may also be very easy to navigate into menus and sub-menus. Indeed, there is no need to calibrate anything since the only parameter that matters is the angle of your arm. Reaching a particular sub-menu could finally be like drawing a path that the user could learn subconciously. 
                </li>
                <li>
                    Drawing selection: the two main advantages of this method is that the user can be in any position, and that it is not tiring since the expanse of the gesture is small. This gesture seems to be great for direct selections. However, contrarily to the previous one, it may be very costly to navigate into sub-menus. Indeed, the user may have to do a "complex" gesture at each step of his progress: he may have to draw the index of each sub-menu. On the contrary, the previous one allowed the user to reach a particular sub-menu in a continuous gesture.
                </li>
            </ul>

        </section>

        <!-- Week 6
        ================================================== -->
        <section id="week6">
            <div class="page-header">
                <h1>Week 6 <small>Milestone 3: Wizard-of-Oz Testing</small></h1>
            </div>

            <p>After week 5, we decided that our goal was to allow the user to get to the content they would like the best with the least amont of effort. To this effect, we imagined a pesonalized TV platform that would keep track of the preferences and habits of its users. We focused on cable TV as our initial application area, since it had significantly less examples of recommendation systems. One assumption we made for each of the prototypes was that there could only be a handful of shows that a user would be intested in at any given time, and we could do a reasonable job in the back-end, predicting which shows these are. We also made a point to have manual override systems in both interfaces to let the users who are seeking specific content easily achieve that.</p>

            <h3>Wizard of Oz 1 - Grid navigation</h3>
            <div class="row">
                <div class="span8 columns">
                    <h4>Use case</h4>
                    Gestures:
                    <ul>
                        <li><b>Highlight:</b> extend your arm toward your target and adjust arm as needed. The highlight-box will move accordingly</li>
                        <li><b>Select:</b> pull hand from wherever it was to chest.</li>
                        <li><b>Back:</b> move hand in a circle-motion (either clockwise or counter-clockwise) </li>
                        <li><b>Home:</b> wave hand at least 3 times</li>
                    </ul>
                    Instructions:
                    <ol>
                        <li>Select Glee //one of the 6 recommendations</li>
                        <li>Go back to recs //quit full screen</li>
                        <li>Browse all //goes to all categories</li>
                        <li>Select a sci-fi movie //somewhat-buried show</li>
                        <li>Go back to selection menu //quit full screen</li>
                        <li>Select a “The Ring” //... another movie in another selection.</li>
                        <li>Go home </li>
                    </ol>
                    <p>We wanted the TV to show the top 6 recommendations for the user. If the user doesn't like the suggestions, he/she can go to "view all", and select the TV shows based that are listed by categories. The flow is shown both from the above insturctions and in the powerpoint document. One of the reasons we decided to go with this design was that it supports browsing a whole library of shows. Indeed, this interface could not only be used for traditional TVs that only allows the user to watch it in real-time, but it couldalso be applicable to the on-demand "smart" TVs such as Google TV, Apple TV. This interface may be more polyvant and scalable.</p>
                    <h4>Lessons Learned</h4>
                    <p>The interface was too slow to navigate and cumbersome. Specific feedback from the grid was that there should be splits between columns and that the category buttons and shows should have different colors. Furthermore, arrows to different categories and shows weren’t clearly visible--and people preferred just to keep navigating “down” rather than having to push an arrow. </p>
                    <p>Beyond interface-specific problems though, it was clear after the first couple users that the gestures did not map to any natural or expected behaviors, and that gestures were more of an obstacle than something that helped. Overall, the gestures seemed force and not easy to remember. We had someone come back to our station, try to use it without us teaching them, and she had already forgotten the selection gesture.</p>
                    <p>On of the mistakes we made while imagining this interface was probably that we wanted it too broad and polyvalent, instead of concentrating on the specific need of people who are watching <strong>cable TV</strong> and who still can't find a proper way to browse the channels and find an interesting show at a given time.</p>
                </div>
                <div class="span4">
                    <ul class="thumbnails">
                        <li class="span4">   
                            <a href="img/original images/woz1-photo.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/woz1-photo.jpg"></img>
                            </a>
                        </li>
                        <li class="span2">
                            <a href="img/original images/woz-grid-menu-1.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/woz-grid-menu-1.jpg"></img>
                            </a>
                        </li>
                        <li class="span2">
                            <a href="img/original images/woz-grid-menu-2.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/woz-grid-menu-2.jpg"></img>
                            </a>
                        </li>
                    </ul>
                    <a class="btn btn-primary" href="download/woz1.pptx">
                        Download the PPT
                    </a>
                </div>
            </div>

            <h3>Wizard of Oz 2 - Radial menu navigation</h3>
            <div class="row">
                <div class="span8 columns">
                    <h4>Use case</h4>
                    <p>Our second gestural navigation prototype was the radial menu. We hypothesized that there can be about 6 different shows going on at a given time in cable TV. Therefore recommendations were a core part of this system. Manually entering a station name/number was left as a last resort.</p>
                    <p>When the user waves at the screen, the first menu they see is the top 6 shows the system thinks s/he would be interested in. We haven't actually worked on the recommendation algorithm but we thought it was reasonable that, based on past habits and a taste profile, we could identify the top shows that are on right now for the user. If these top 6 suggestions were not adequate though, the user can navigate to more specific categories and genres for further recommendations. Put differently, this interface is made for people who are in the following range: "I don't know at all what I want to watch so propose me something" to "I know quite accurately what I want to watch (I could sort that into categories) but not exaclty, so propose me something in that narrowed set of possibilities".</p>
                    <p>And finally, for those who know exactly what they want to watch, we also included a "Direct Input" menu. In this menu, people would start typing the name of a show by drawing the first letters with his hand, and the list of matching results would get updated just like the instant search in Google. With a good algorithm, we could expect to reach the desired answer in less than 3 letters.</p>
                    <p>We had one design for this interface but we tested two different interaction methods with it:
                        <ul>
                            <li>The first UX method used a grab&select approach. To select an item or jump to a sub-category in a list the user could point at it with their open hand, then close their hand and pull it back to themselves. Similarly, to go back from a sub-category to a parent category the user could first close their hand (grasp) and then push back towards the screen.</li>
                            <li>The second method involed a free-moving viewport based on the user's motions. In this one, once the menu showed up, the path that the user takes with their arm would be replicated as the path that the viewport takes on the radial structure on the screen. This way, without the extra steps for selecting items, the user could simply skip to a leaf of the tree in one smooth motion, if they had internalized the gesture.</li>
                        </ul>

                    </p>
                    <h4>Lessons Learned</h4>
                    <p>The first piece of feedback we got was that this menu paradigm was preferable to the grid menu in a variety of ways. Its interaction was more predictable–moving around in a tree was a reasonable metaphor for the CS students that we tested with–--and simpler.</p>
                    <p>With Rob's advice, we tried to test the prototypes with our subjects with little to no instruction in order to observe how they were acting (and try to understand why). This gave us some very interesting results. For instance, subjects tried to:</p>
                        <ul>
                            <li>flick</li>
                            <li>pinch to zoom in a show</li>
                            <li>rotate the interface</li>
                            <li>swipe to navigate into the recommendations</li>
                        </ul>
                    <p>These are all gestures that are already familiar to them from touch screens or trackpads. We also realized that we could incorporate some other gestures such as a go back to root gesture (pushing the screen away with both hands). All those suggestions and subconscious acts should orient the next iterations of this interface.</p>
                </div>

                <div class="span4">
                    <ul class="thumbnails">
                        <li class="span2">
                            <a href="img/original images/woz2-sketch.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/woz2-sketch.jpg"></img>
                            </a>
                        </li>
                        <li class="span2">
                            <a href="img/original images/woz2-photo.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/woz2-photo.jpg"></img>
                            </a>
                        </li>
                        <li class="span4">
                            <a href="img/original images/woz-radial-menu.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/woz-radial-menu.jpg"></img>
                            </a>
                        </li>
                    </ul>
                    <a class="btn btn-primary" href="download/woz-radial-menu.zip">
                        Download full size
                    </a>
                </div>
            </div>

            <div>
                <h3>Overall Lessons Learned</h3>
                <p>After a lengthy talk with Rob and then another help session with Anita, our coach, we learned how to think of the Kinect and our product as not just an ‘interface’ but get back to the target user and use-case. We learned that using abstract information visualization is OK and designers do it all the time, but before we decide on an navigable interface we should really focus on what the user is trying to do (Rob brought up the example that if we somehow came up with a way to “read minds” and our TV could then offer recommendations based on that, that would be completely acceptable--and interface-free!), which is something we missed a little during the past couple of week. <strong>We intend to catch up with this process in the very near future.</strong></p>

                <p>Along those lines, we decided to think of other implicit was that the Kinect could offer recommendations. As much as a literal “list” of TV shows may be helpful, we tried thinking about the user as someone in their living room watching TV. We thought if the Kinect could use your actions as contextual clues for what shows you would want, we wouldn’t need to force or teach any gestures at all!</p>

                <p>For example, a Kinect can tell if you’re eating some snack for lunch (and for example display a summary of the news in less than 15 min); or it can detect if you are interrupted by a phone call or if you are texting someone during an important phasis of a game (and pause the program temporarily); or if you are leaving for a short amount of time (we could have a cue "I'll be right back", which is what people say when they are in a group); or if you’re alone and relaxing, or if you’ve just come back from working out, or if no one is watching and the TV is just background noise. (We came up with a long list, even... horizontal with someone else or... if you’re pants are off!) In each of those cases, there’s an expected context that the TV lives in--and we want to be able to recognize those cases. Then, based on your actions, the TV can change accordingly to fit the current situation.</p>
                
                <p>We are planning to support (or invalidate) those hypothesis with several rounds of observation of people watching TV all over the week-end. Hopefully, this will not only confirm this direction but it would also provide us some cases of use we haven't thought of yet.</p>
            </div>

    </section>

    <!-- Week 7
    ================================================== -->
    <section id="week7">
        <div class="page-header">
            <h1>Week 7 <small>Milestone 4: Functional Prototype I</small></h1>
        </div>
		<div >
        <ul class="thumbnails">
                        <li class="span4">
                            <a href="img/original images/prototype1.jpg" class="thumbnail" target='_blank'>
                                <img src="img/thumbnail images/prototype1.jpg"></img>
                            </a>
                        </li>
                    </ul>
        </div>
		
		<div>
        <h3>Motives, Learnings and Selected Features</h3>
        Main goal: have the TV cater to the user's needs in any context with as little human effort as possible. We wanted to come up with a basic UI that reacts to the actions the user naturally takes. We observed TV watchers to see what are the sommon things people consciously or subconsciously do in front of a TV, such as answering the phone, getting up to grab a drink, etc. Our goal is to make the aware of these actions via the Kinect and react to them so that the user does not need to do any additonal gestures or interactions. <p>

        <h4>Turning on the TV</h4>
        Sit back and wave. Originally, we prototyped such that the TV turns on and plays whenever the user sits down. However, we realized that people may not always want to have the TV turn on when they are sitting in the living room. We then decided to add in the smallest and most intuitve piece of action our users would want to do to turn on, waving! The waving motion aligns with our goal that is does not add too much hassle on the user's part, but also eliminates false positives as it is not very common to wave at the TV. <p>           

        <h4>Content Selection</h4> 
        Based on interviews, we noticed a few common reasons people choose to watch traditional TV as opposed to on demand content on their computers:
        <ul>
            <li>
            The quick and effortless nature of getting to content on a TV. Sitting down and turning on the TV quickly gets the user to <i>some</i> content. Picking the exact best content for that moment is very often not worth the effort for most people. 
            </li>
            <li>
            Because TV has a global temporal axis, chances of friends/family to be watching or to have seen the same show with you at the same time is much higher. This way, people can talk about the same content with their friends. We realized that people are more interested in reading an article, listening to a song that their friends are listening to.
            </li>
        </ul>

        To support the above use cases as well as possible, we made the following content selection flow. :
        <ul>
        <li>If you (ie the same user, face recognition was mocked for now) were recently (ie less than 2 hrs ago) watching a show, then the same show resumes when you turn on the TV again. The user can still get to other content with an air swipe.</li>
        
        <li>If you are not resuming a show, or if you asked for different content with a swipe, we jump to what a friend (again, selection of exactly which friend is not implemented) is currently watching. You can keep swiping to browse further friends. This is like coming home to finding a <i>virtual</i> roommate wathcing a certain TV channel, except the user also has the freedom to browse other virtual roommates if desired. To enhance the virtual roommate experience, we also relayed some of the reactions watchers gave to the current content to other watchers. This meant that if you are watchign a show with Jeff, you would see a notification on the screen when Jeff laughs at a joke.</li>

        <li>If you have the energy to manually pick content, you can also pick up the remote and override YouMote. Your previously set preferences will determine if others can follow your viewing on their YouMotes.</li>
		</ul>        

        <h4>Pausing the show</h4>
		<p>A key insight we had was that users rarely every get up from their seats while still continuing to watch the TV. Therefore, we pause the TV when the user gets up and stays up for more than a very short amount of time.</p> 

		<h4>Turning off the TV</h4>
		<p>As a continuation of pausing the show when the user gets up, if the user leaves and doesn't come back for a certain amount of time (ie 15 min) the TV will shut down. This makes the user think about one less step, and also helps them continue watching the content when they come back.
		</p>
        
        <h4>Answering the phone</h4>
        <p>If a user gets a call, we would likely turn off the volume or pause the program. In our prototype, we have built the detector for when the user has one hand close to their head. We are also experimenting with audio detection, so when the user says "hello" or constantly talks with one of their hand on their head, we can then lower the volume of the show or pause the show.</p>
        
        <h4>Sharing responses for the content</h4>
	<p>In our prototype, we hardcoded the social experience where the user can see his/her friend's reaction during a particular part of the show. When there is a no-so-funny joke, you can see a small note on the TV that shows that Jeff still laughed "Hahahahahahaha" during that part. We think that not only are people more likely to enjoy the TV shows that their friends are watching, they may also be pleased to virtually share the experience with their friends, even if they can't be physically colocated.
	</p>
        
        <h3>Current Implementation Progress</h3>
		
		<ul>
			<li>When the user sits down, the video comes on and plays the show recommended to the user.
			</li>
			
			<li> When the user stands up, the video pauses.
			</li>

			<li> When the user leaves the scope of the kinect camera for a certain amount of time, the TV shuts down.
			</li>

			<li> When the user watches the show, he/she can see the parts where his friend, Jeff, makes a comment or laughs (hard-coded).
			</li>
			
			<li> When the user puts his hand on his head, the TV detects that the user may be talking on the phone and displays that on the screen (no subsequent action yet).
			</li>
			
            <li>
            The system also notes the phrases the user utters (for future use in reaction detection or to enhance the phone classifier.)
            </li>

			</ul>
        
        <h3>Remaining implementation and design issues</h3>
        <ul>
        <li>Figure out how to best relay reactions in a potentially asynchronous setting due to pauses, or see if it's even necessary (P1 - see our thoughts below)</li>
        <li>Implement the outstanding gesture classifiers - Waving, air swiping, etc. (P1)</li>
        <li>Move current code to the new Kinect SDK to use premade gesture classifiers (P1)</li>
        <li>Come up with a streaming video source to better simulate TV (P2)</li>
        <li>Figure out the set of reactions we would like to relay (P3)</li>
        <li>Implement the classifiers for those reactions (P3)</li>
        </ul>

        <p>After the testing in class and further discussions, we laid out the above interaction pattern and will need to see it fully implemented.</p>
        <p>The main remaining issue is the integration of the social interactions. This is a part we are currently debating on. There are several options that mainly depend on the kind of TV we are watching.</p>
        <p>For cable TV, the interaction would be real time: people are watching a show live and their interactions are transcribed on the screen at the moment they are doing them. This solution has several issues. First, how to manage pausing when people are standing? For instance, we could have a default behavior that would not pause the show <em>unless</em> the person who is standing (to go to the toilet, to grab something to drink) says an audio cue like "please wait for me", which is something that people are already doing when they are watching TV in group (in the same room). Talking about groups, this solution would in fact imply that people forming a virtual group on purpose in order to watch TV together. Indeed, if this was not the case, people could be bothered if someone joins their channel and starts pausing at every moment whereas he didn't want to be disturbed. An other solution could be to override the pause in order to start asynchronous watching, which leads us to the second solution.</p>
        <p>The other solution would work for VOD: people are not watching the show at same time but all their comments/interatcions are recorded. Then, when someone watches the same show (but it can be even 2 weeks later), those interactions/reactions could displayed in a corner of the screen, to give the impression that you are watching the show with one of your friend. This solution gives much more liberty in what it is possible to do, but on the other hand, it loses an important part of the social aspect of TV: I am happy to watch something when I know that someone else is watching it exactly at the same time even in a different place (think of the SuperBall: I know I can talk about that at work the day after, etc.), but this feeling is weakened if I know the person is not watching it at the same time.</p>
        <p>To determine what's best, we intend to make another round of observations of people watching TV in group. If we choose the live cable TV solution, maybe it would be even better to include audio so that people can comment and make jokes even more naturally. On the other hand maybe a well thought asynchronous social interaction would fulfill the social needs or urges of people when they are watching TV.</p>
		</div>
	
	            
	</section>

    <!-- Week 8
    ================================================== -->
    <section id="week8">
        <div class="page-header">
            <h1>Week 8 <small>Milestone 5: User Testing Results</small></h1>
        </div>
		<p>We have written the Introduction and and methods in a google doc at this <a href="https://docs.google.com/document/d/1Yiu8ySIDTXJXlLIi8pyvqR87FS4R0Q8sZrxibtzcKOc/edit">link</a>.</p>
		
    </section>

    <!-- Week 9
    ================================================== -->
    <section id="week9">
        <div class="page-header">
            <h1>Week 9 <small>Milestone 6: Functional Prototype II</small></h1>
        </div>
    </section>

    <!-- Week 10
    ================================================== -->
    <section id="week10">
        <div class="page-header">
            <h1>Week 10 <small>Milestone 7: Demo and Presentation</small></h1>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <p class="pull-right"><a href="#">Back to top</a></p>
            <p>
                <strong>CS247 Project 4</strong><br />
                <a href="./team.html#arda">Arda Kara</a> &middot; <a href="./team.html#kenny">Kenny Kao</a> &middot; <a href="./team.html#robi">S&eacute;bastien Robaszkiewicz</a> &middot; <a href="./team.html#charlton">Charlton Soesanto</a> &middot; <a href="./team.html#blake">Blake Carpenter</a>
            </p>
        </div>
    </footer>
</div> <!-- Container -->

<!-- Le javascript
================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
<script src="http://twitter.github.com/bootstrap/assets/js/jquery.js"></script>
<script src="http://twitter.github.com/bootstrap/assets/js/google-code-prettify/prettify.js"></script>
<script src="http://twitter.github.com/bootstrap/assets/js/bootstrap-transition.js"></script>
<script src="js/bootstrap-alert.js"></script>
<script src="js/bootstrap-modal.js"></script>
<script src="js/bootstrap-dropdown.js"></script>
<script src="js/bootstrap-scrollspy.js"></script>
<script src="js/bootstrap-tab.js"></script>
<script src="js/bootstrap-tooltip.js"></script>
<script src="js/bootstrap-popover.js"></script>
<script src="js/bootstrap-button.js"></script>
<script src="js/bootstrap-collapse.js"></script>
<script src="js/bootstrap-carousel.js"></script>
<script src="js/bootstrap-typeahead.js"></script>
<script src="js/application.js"></script>

</body>
</html>
